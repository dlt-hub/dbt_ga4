from typing import List

import dlt
from dlt.common import pendulum
from google.cloud import bigquery
from google.oauth2 import service_account
from tqdm import tqdm


# the report is generated by default for the previous month
# if a specific month is not specified
today = pendulum.now()
if today.month > 1:
    last_month = today.month - 1
    year = today.year
else:
    last_month = 12
    year = today.year - 1


@dlt.source
def bigquery_source(credentials_info=dlt.secrets.value, tables=(), month=last_month, year=year):
    for table in tables:
        return dlt.resource(
            bigquery_table_resource,
            name=table,

        )(credentials_info, table, month, year)


def bigquery_table_resource(credentials_info, table=None, month=last_month, year=year):
    """
        Loads all the GA events data for the period of 1 month
        If the month is not explicitly passed, then it loads the data for the previous month
    """
    # credentials are read from .dlt/secrets.toml
    credentials = service_account.Credentials.from_service_account_info(credentials_info)
    client = bigquery.Client(credentials=credentials)

    # In 'project_name.dataset_name.events_*', replace 'project_name' with the name of your bigquery project,
    # 'dataset_name' with the name of the google analytics dataset
    query_str = f"""
        select * from `{credentials_info['project_id']}.{credentials_info['dataset_name']}.{table}_*` 
        where _table_suffix between format_date('%Y%m%d',cast('{year}-{month}-1' as date)) 
                                and format_date('%Y%m%d',date_add(cast('{year}-{month}-1' as date), interval 1 month))
    """

    for row in tqdm(client.query(query_str), desc="Loading data..."):
        yield {key: value for key, value in row.items()}


if __name__ == "__main__":
    # Load tables with the standalone table resource
    data_source = bigquery_source(tables=["events"])

    # configure the pipeline with your destination details
    pipeline = dlt.pipeline(
        pipeline_name='bigquery_pipeline', destination='bigquery', dataset_name='test_alena'
    )
    # run the pipeline with your parameters
    load_info = pipeline.run(data_source)
    # pretty print the information on data that was loaded
    print(load_info)

    # make or restore venv for dbt, using latest dbt version
    # NOTE: if you have dbt installed in your current environment, just skip this line
    #       and the `venv` argument to dlt.dbt.package()
    venv = dlt.dbt.get_venv(pipeline)

    # get runner, optionally pass the venv
    dbt = dlt.dbt.package(
        pipeline,
        "dbt_transform",
        venv=venv
    )

    # run the models and collect any info
    # If running fails, the error will be raised with full stack trace
    models = dbt.run_all(additional_vars={"database_name": "dlthub-analytics", "dataset_name": "test_alena"})

    # on success print outcome
    for m in models:
        print(
            f"Model {m.model_name} materialized " +
            f"in {m.time} " +
            f"with status {m.status} " +
            f"and message {m.message}"
        )